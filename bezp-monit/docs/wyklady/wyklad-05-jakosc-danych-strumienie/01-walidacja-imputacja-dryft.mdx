---
title: "Walidacja, imputacja, dryft czujnikÃ³w"
sidebar_position: 1
---

import { 
  SlideContainer, 
  Slide, 
  KeyPoints, 
  SupportingDetails, 
  InstructorNotes,
  VisualSeparator,
  LearningObjective,
  KeyConcept,
  Example
} from '@site/src/components/SlideComponents';
import { InteractiveQuiz } from '@site/src/components/InteractiveQuiz';

<LearningObjective>
Po tej sekcji student potrafi:
- ZaprojektowaÄ‡ reguÅ‚y walidacji danych (range checks, physical laws, cross-channel consistency)
- ZastosowaÄ‡ odpowiednie metody imputacji (interpolacja, forward-fill, model-based) z transparency flagging
- WykryÄ‡ drift czujnikÃ³w uÅ¼ywajÄ…c metod statystycznych (CUSUM, MAD, reference comparison)
- ZaimplementowaÄ‡ pipeline jakoÅ›ci danych (validation â†’ imputation â†’ quality scoring) w systemie monitoringu
</LearningObjective>

<SlideContainer>

<Slide title="ðŸ§ª Walidacja danych â€“ 4 wymiary jakoÅ›ci" type="info">

<KeyPoints title="ðŸ“‹ Framework jakoÅ›ci danych (Data Quality Dimensions)">

**ISO 8000** i literatura (Batini, Scannapieco) definiujÄ… **4 kluczowe wymiary**:

**1. Completeness (kompletnoÅ›Ä‡)**
- **Definicja**: JakÄ… czÄ™Å›Ä‡ expected data points faktycznie mamy?
- **Formula**: $Completeness = \frac{N_{received}}{N_{expected}} \times 100\%$
- **Target**: &gt;95% dla critical KPI (PR, billing), &gt;90% dla monitoring
- **PrzykÅ‚ad**: Farma PV, 1 miesiÄ…c, 15-min intervals â†’ expected 2880, received 2850 â†’ 98.96% âœ“

**2. Accuracy (dokÅ‚adnoÅ›Ä‡)**
- **Definicja**: Jak blisko prawdziwej wartoÅ›ci (zgodnoÅ›Ä‡ z rzeczywistoÅ›ciÄ…)?
- **Measurement**: NiepewnoÅ›Ä‡ pomiarowa (z kalibracji), bÅ‚Ä…d systematyczny
- **Target**: Depends on czujnika (pyranometr First Class Â±5%, PT1000 Â±0.3Â°C)
- **PrzykÅ‚ad**: G_POA = 950 W/mÂ² Â± 20 W/mÂ² (U, k=2) â†’ accuracy band

**3. Consistency (spÃ³jnoÅ›Ä‡)**
- **Definicja**: ZgodnoÅ›Ä‡ miÄ™dzy related data points (nie contradictions)
- **Types**:
  - **Internal**: âˆ‘I_string â‰ˆ I_DC_inverter (suma prÄ…dÃ³w stringÃ³w = prÄ…d total)
  - **Temporal**: P(t) nie moÅ¼e jump z 1000 kW do 5000 kW w 1 s (physical limit)
  - **Cross-source**: G_POA(pyranometr) vs. G_POA(satelita) powinny korelowaÄ‡ (RÂ² >0.9)
- **Example**: JeÅ›li P_AC = 2000 kW, ale G_POA = 10 W/mÂ² (noc) â†’ INCONSISTENT (likely sensor fault)

**4. Timeliness (aktualnoÅ›Ä‡)**
- **Definicja**: Jak "Å›wieÅ¼e" sÄ… dane (opÃ³Åºnienie acquisition â†’ availability)
- **Target**: &lt;10 s dla real-time control, &lt;5 min dla monitoring
- **PrzykÅ‚ad**: SOE event timestamp 14:23:42.123, received @ SCADA 14:23:45.678 â†’ latency 3.5 s âœ“

</KeyPoints>

<SupportingDetails title="ðŸ”§ ReguÅ‚y walidacji â€“ hierarchia checks">

### **Level 1: Range checks (podstawowe)**

**Min/max bounds (zakresy fizyczne):**

```python
# Python validation example
def validate_range(value, min_val, max_val, param_name):
    if value < min_val or value > max_val:
        return False, f"{param_name} out of range: {value} not in [{min_val}, {max_val}]"
    return True, "OK"

# Examples:
validate_range(G_POA, min=0, max=1500, param="Irradiance")  # W/mÂ², max theoretical ~1350
validate_range(T_mod, min=-40, max=85, param="Module Temp")  # Â°C, operational range
validate_range(P_AC, min=0, max=P_rated*1.05, param="AC Power")  # Allow 5% overload transient
validate_range(SoC, min=0, max=100, param="State of Charge")  # %
validate_range(f_grid, min=47, max=52, param="Grid Frequency")  # Hz, Europe 50 Hz Â±2 Hz extreme
```

**Typowe zakresy dla wielkoÅ›ci OZE:**

| WielkoÅ›Ä‡ | Min | Max | Uwagi |
|----------|-----|-----|-------|
| **G_POA** | 0 W/mÂ² | 1500 W/mÂ² | &gt;1350 = suspekt (odbicia, bÅ‚Ä…d czujnika) |
| **T_mod** | -40 Â°C | 85 Â°C | PoniÅ¼ej/powyÅ¼ej = ekstremalnie rzadkie |
| **P_AC** (PV) | 0 W | P_rated Ã— 1.05 | Overload transient dopuszczalny &lt;1 min |
| **Wind speed** | 0 m/s | 40 m/s | &gt;35 m/s = shutdown (safety) |
| **SoC** (BESS) | 0% | 100% | Exactly, nie moÅ¼e byÄ‡ &lt;0 lub &gt;100 |
| **U_grid** | 0.9 Ã— U_nom | 1.1 Ã— U_nom | Dla 110 kV: 99-121 kV |
| **f_grid** | 47.5 Hz | 52.5 Hz | Europe 50 Hz, extreme Â±2.5 Hz |

---

### **Level 2: Physical laws (reguÅ‚y fizyczne)**

**Relationship checks:**

```python
# P_AC nie moÅ¼e przekraczaÄ‡ P_DC (inverter efficiency <100%)
def validate_power_balance(P_AC, P_DC, tolerance=0.02):
    if P_AC > P_DC * 1.02:  # Allow 2% measurement error
        return False, f"P_AC ({P_AC}) > P_DC ({P_DC}) â€“ impossible (efficiency >100%)"
    return True, "OK"

# P_AC â‰¤ P_theoretical (function of G, T)
def validate_pv_physics(P_AC, G_POA, T_mod, P_rated, gamma=-0.004):
    # Theoretical max (simplified, ignores efficiency losses)
    P_theoretical = P_rated * (G_POA / 1000) * (1 + gamma * (T_mod - 25))
    
    if P_AC > P_theoretical * 1.1:  # Allow 10% margin (measurement uncertainty)
        return False, f"P_AC ({P_AC}) >> P_theoretical ({P_theoretical}) â€“ inconsistent"
    return True, "OK"

# Energy conservation (nie moÅ¼na wytworzyÄ‡ wiÄ™cej niÅ¼ dostÄ™pne w ÅºrÃ³dle)
# For BESS: E_discharged â‰¤ E_charged Ã— round_trip_efficiency
def validate_bess_energy(E_discharged, E_charged, eta_rt=0.90):
    if E_discharged > E_charged * eta_rt * 1.05:  # 5% margin
        return False, f"Discharged ({E_discharged}) > Charged ({E_charged}) Ã— Î· â€“ energy loss too high"
    return True, "OK"
```

---

### **Level 3: Cross-channel consistency (spÃ³jnoÅ›Ä‡ miÄ™dzykanaÅ‚owa)**

```python
# Sum of string currents â‰ˆ DC current inverter
def validate_string_currents(I_strings, I_DC_total, tolerance=0.05):
    I_sum = sum(I_strings)
    diff_percent = abs(I_sum - I_DC_total) / I_DC_total
    
    if diff_percent > tolerance:  # >5% difference
        return False, f"String currents sum ({I_sum} A) â‰  DC total ({I_DC_total} A), diff {diff_percent*100:.1f}%"
    return True, "OK"

# Wind: Generator speed â‰ˆ Rotor speed Ã— gearbox ratio
def validate_wind_gearbox(rotor_rpm, gen_rpm, ratio=91, tolerance=0.05):
    expected_gen = rotor_rpm * ratio
    diff_percent = abs(gen_rpm - expected_gen) / expected_gen
    
    if diff_percent > tolerance:
        return False, f"Generator speed ({gen_rpm} rpm) â‰  Rotor ({rotor_rpm}) Ã— ratio ({ratio}), possible gearbox fault"
    return True, "OK"
```

---

### **Quality flags (znaczniki jakoÅ›ci):**

Po walidacji, assign quality flag:
- **GOOD** (0x00): All validations passed
- **SUSPECT** (0x04): Failed Level 2/3 (physical law, consistency), ale nie Level 1 (range)
- **BAD** (0x01): Failed Level 1 (out of range, clearly invalid)
- **MISSING** (0xFF): No data received

</SupportingDetails>

<Example title="Walidacja pipeline â€“ farma PV 10 MWp">

**Architektura:**

```mermaid
graph LR
    A[Raw Data<br/>Modbus/MQTT<br/>100 Hz] --> B[Level 1<br/>Range Check]
    B -->|PASS| C[Level 2<br/>Physics Check]
    B -->|FAIL| F[Flag: BAD<br/>Log rejection]
    
    C -->|PASS| D[Level 3<br/>Consistency Check]
    C -->|FAIL| G[Flag: SUSPECT<br/>Log warning]
    
    D -->|PASS| E[Flag: GOOD<br/>Store to DB]
    D -->|FAIL| G
    
    E --> H[InfluxDB<br/>with quality field]
    F --> I[Rejection Log<br/>for analysis]
    G --> H
    
    style B fill:#ffffcc
    style C fill:#ffeb99
    style D fill:#ffcc99
    style E fill:#90ee90
    style F fill:#ff9999
    style G fill:#ffcccc
```

**Statistics (1 miesiÄ…c, 2 880 000 data points total):**

| Quality | Count | Percentage | Action |
|---------|-------|------------|--------|
| **GOOD** | 2 835 600 | 98.46% | Used in KPI calculations |
| **SUSPECT** | 28 800 | 1.00% | Logged, used with caution (flagged) |
| **BAD** | 14 400 | 0.50% | Rejected (not stored) |
| **MISSING** | 1 200 | 0.04% | Gaps (imputation moÅ¼e byÄ‡ applied) |

**Rejection reasons (BAD, top 5):**
1. G_POA < 0 W/mÂ² (6000 points, 0.21%) â€“ sensor fault (night offset drift)
2. P_AC > P_rated Ã— 1.05 (4800 points, 0.17%) â€“ measurement spike (EMI)
3. T_mod > 85 Â°C (2400 points, 0.08%) â€“ sensor fault lub extreme heat (investigated)
4. f_grid > 52 Hz (800 points, 0.03%) â€“ grid event (external, confirmed by OSD)
5. Communication timeout (400 points, 0.01%) â€“ network glitch

**Actions taken:**
- G_POA offset: Recalibration pyranometru (zero adjustment)
- P_AC spikes: Added EMI filter (RC, 100 Hz cutoff)
- T_mod >85Â°C: False alarm (sensor briefly touched by bird, no real overheat)

**Impact on KPI:**
- **Przed walidacjÄ…**: PR calculated z ALL data (incl. BAD) = 79.2% (zaniÅ¼ony przez night G_POA &lt;0)
- **Po walidacji**: PR tylko GOOD data = **82.8%** (correct value)
- **Difference**: 3.6 punktÃ³w procentowych! Walidacja kluczowa.

</Example>

<InstructorNotes>

**Czas**: 16-18 min

**Przebieg**:
1. 4 wymiary jakoÅ›ci (4 min) â€“ completeness, accuracy, consistency, timeliness
2. Hierarchia walidacji (5 min) â€“ Level 1 (range), Level 2 (physics), Level 3 (consistency)
3. Pipeline diagram (2 min) â€“ pokazuje flow
4. Case study (4 min) â€“ real statistics, rejection reasons, impact on KPI
5. Q&A (2 min)

**Punkty kluczowe**:
- **"Garbage in, garbage out"** â€“ bez walidacji KPI sÄ… worthless
- **Multi-level validation** (nie tylko range!) â€“ physics i consistency catch subtle errors
- **Quality flags obowiÄ…zkowe** â€“ kaÅ¼dy data point MA quality (GOOD/SUSPECT/BAD/MISSING)
- **Impact na KPI moÅ¼e byÄ‡ huge** (3.6 p.p. difference w PR!)

**Demonstracja praktyczna**:
- Live validation (Python script) â€“ input bad data, pokazuje rejection
- InfluxDB query z quality filtering: `WHERE quality='GOOD'`
- Wykres: Before/after validation (PR trend, pokazuje skok gdy fix validation)

**MateriaÅ‚y pomocnicze**:
- ISO 8000 (Data Quality) â€“ framework
- Great Expectations (Python library) â€“ data validation framework
- Example validation rules (YAML config file)

**Typowe bÅ‚Ä™dy studenckie**:
- Tylko range validation (Level 1) â€“ insufficient, physics checks kluczowe
- Brak quality flags (all data treated as GOOD) â€“ risky
- Storing rejected data (zamiast log separately) â†’ pollutes DB

**Pytania studenckie**:
- Q: Czy zawsze reject data jeÅ›li fails validation?
- A: Depends on level. Level 1 (range) = hard reject (clearly invalid). Level 2/3 (physics, consistency) = flag as SUSPECT (moÅ¼e byÄ‡ measurement error, ale moÅ¼e teÅ¼ transient real event). Human review dla SUSPECT.

- Q: Co z night data (G_POA = 0, P_AC = 0)?
- A: NIE reject (to valid!). Ale: exclude z PR calculation (IEC 61724 wymaga G > 200 W/mÂ² dla PR). Filter in query, not in validation.

</InstructorNotes>

</Slide>

<VisualSeparator type="technical" />

<Slide title="ðŸ§© Imputacja brakÃ³w (Missing Data Imputation)" type="tip">

<KeyConcept title="Kiedy i jak imputowaÄ‡ missing data?">

**Missing data** wystÄ™puje z powodu:
- Communication errors (network glitches, Modbus timeout)
- Sensor faults (temporary failure, restoration)
- Maintenance windows (planned downtime)
- Power outages (UPS exhausted, &lt;5 min typical)

**Decision tree: Czy imputowaÄ‡?**

```mermaid
graph TD
    A[Missing data<br/>detected] --> B{Duration<br/>of gap?}
    
    B -->|<1 min| C[Interpolacja liniowa<br/>flag: INTERPOLATED]
    B -->|1-15 min| D{Data type?}
    B -->|>15 min| E[NO imputation<br/>flag: MISSING<br/>Exclude from KPI]
    
    D -->|Slow process<br/>temp, SoC| F[Forward-fill<br/>flag: FILLED]
    D -->|Fast process<br/>power, wind| G[Model-based<br/>or MISSING]
    
    C --> H[Store with<br/>quality flag]
    F --> H
    G --> H
    E --> I[Gap in data<br/>log incident]
    
    style C fill:#90ee90
    style F fill:#ffeb99
    style G fill:#87ceeb
    style E fill:#ffcccc
```

### **Metody imputacji:**

**1. Linear interpolation (interpolacja liniowa)**

**Zastosowanie**: Short gaps (&lt;1 min), smooth processes
**Formula**: 
$$
y(t) = y_{t-1} + \frac{y_{t+1} - y_{t-1}}{t+1 - (t-1)} \times (t - (t-1))
$$

**PrzykÅ‚ad:**
```python
# Gap: t=10:05:00 missing (between 10:04:00 and 10:06:00)
t_before = "10:04:00", value = 950 W/mÂ² (G_POA)
t_after  = "10:06:00", value = 970 W/mÂ²

# Interpolate @ 10:05:00 (midpoint)
value_imputed = 950 + (970 - 950) / 2 = 960 W/mÂ²
quality = "INTERPOLATED"
```

**Zalety**: Prosty, szybki, reasonable dla smooth signals  
**Wady**: Assumes linearity (moÅ¼e miss peaks/valleys w gap)

---

**2. Forward-fill (carry forward last known value)**

**Zastosowanie**: Slow-changing processes (SoC, temp amb), gaps 1-15 min

**PrzykÅ‚ad:**
```python
# Gap @ 10:05:00, last known SoC @ 10:04:00 = 65.2%
# Assume SoC nie zmieniÅ‚ siÄ™ drastycznie w 1 min
SoC_imputed = 65.2%  # Same as last
quality = "FILLED"
```

**Zalety**: Safe assumption dla slow processes  
**Wady**: Ignores changes w gap (jeÅ›li byÅ‚o charging/discharging, bÄ™dzie wrong)

---

**3. Model-based imputation (predictive)**

**Zastosowanie**: Longer gaps, gdy mamy model procesu

**PrzykÅ‚ad (PV power based on irradiance model):**
```python
# Gap: P_AC missing @ 10:05:00
# Available: G_POA = 950 W/mÂ², T_mod = 55Â°C
# Model: P_AC = P_rated Ã— (G/1000) Ã— (1 + gamma Ã— (T-25)) Ã— efficiency

P_imputed = 10000 Ã— (950/1000) Ã— (1 + (-0.004) Ã— (55-25)) Ã— 0.95
          = 10000 Ã— 0.95 Ã— 0.88 Ã— 0.95
          = 7942 kW
quality = "MODEL_IMPUTED"
```

**Zalety**: Physics-informed, can be accurate  
**Wady**: Model uncertainty (moÅ¼e be wrong jeÅ›li assumptions violated)

---

**4. NO imputation (gdy gap zbyt dÅ‚ugi lub critical data)**

**Przypadki:**
- Gap >15 min â†’ flag as MISSING, exclude z KPI
- SOE events (NEVER imputuj â€“ kaÅ¼dy event musi byÄ‡ real, nie estimated)
- Billing data (energy totals) â€“ conservative approach (don't invent data)

</KeyConcept>

<SupportingDetails title="ðŸŽ¯ Best practices dla imputacji">

### **Zasady transparentnoÅ›ci:**

**1. ZAWSZE flaguj imputowane dane**
- Original data point: `value=950, quality=GOOD`
- Imputed data point: `value=960, quality=INTERPOLATED`
- W InfluxDB:
```flux
// Query: Exclude imputed data from KPI
from(bucket: "solar")
  |> filter(fn: (r) => r.quality == "GOOD")  // Only measured, nie imputed
```

**2. Przechowuj original gaps (nie overwrite)**
- Option A: Separate field (`value` + `value_imputed`)
- Option B: Quality flag distinguishes (query filters)

**3. Dokumentuj imputation policy**
```yaml
# Imputation config (YAML)
imputation_rules:
  G_POA:
    gap_threshold_sec: 60  # <60s â†’ interpolate
    method: linear
  SoC:
    gap_threshold_sec: 900  # <15 min â†’ forward-fill
    method: forward_fill
  P_AC:
    gap_threshold_sec: 300  # <5 min â†’ model-based
    method: physics_model
    model_params:
      efficiency: 0.95
      gamma: -0.004
  SOE_events:
    method: none  # NEVER impute events
```

**4. Monitor imputation rate**
$$
Imputation\_rate = \frac{N_{imputed}}{N_{total}} \times 100\%
$$

**Target**: &lt;2% (jeÅ›li &gt;5% â†’ investigate communication/sensor issues)

</SupportingDetails>

<Example title="Impact imputacji na KPI â€“ case study">

**Scenariusz: Communication loss, farma PV 5 MWp, 15 min gap**

**Gap details:**
- Time: 10:00-10:15 (15 min = 1 interval w 15-min aggregation)
- Cause: Fiber cut (construction accident)
- Missing: P_AC, G_POA, T_mod (all sensors offline)

**Option 1: NO imputation (conservative)**
- Energy month: 2845 MWh - **15 min gap excluded**
- Gap energy estimate (missed): ~1.2 MWh (5 MW Ã— 0.25 h)
- **Billed energy**: 2843.8 MWh (undercounted -0.04%)
- Financial impact: -1.2 MWh Ã— â‚¬50/MWh = **-â‚¬60** (lost revenue, nie moÅ¼na bill)

**Option 2: Model-based imputation (aggressive)**
- Use: Weather forecast (G predicted from satellite) + temperature model
- Estimated P_AC: 4.8 MW (based on model)
- Gap energy imputed: 4.8 MW Ã— 0.25 h = 1.2 MWh
- **Billed energy**: 2845 MWh (imputed included, flagged)
- Financial impact: **â‚¬0** (no loss)

**Risk:**
- Model uncertainty: Â±10-20% (weather forecast can be wrong)
- Audit challenge: OSD moÅ¼e not accept imputed data dla billing (depends on contract)

**Best practice (hybrid):**
- **For internal KPI**: Use imputation (better estimate)
- **For billing/compliance**: Conservative (NO imputation, flag gap, accept small loss)
- **Documentation**: Report gap w monthly KPI ("15-min communication loss on 8 July, 1.2 MWh estimated loss, excluded from billing per SLA")

**Lesson**: Imputation to trade-off â€“ accuracy vs. completeness. Transparency (flagging) kluczowa.

</Example>

<InstructorNotes>

**Czas**: 14-16 min

**Przebieg**:
1. Kiedy imputowaÄ‡ decision tree (3 min)
2. Metody imputation (5 min) â€“ linear, forward-fill, model-based, none
3. Best practices (3 min) â€“ transparency, quality flags, documentation
4. Case study (3 min) â€“ pokazuje trade-off i financial impact
5. Q&A (2 min)

**Punkty kluczowe**:
- **Imputation to NIE zawsze good idea** â€“ short gaps OK, long gaps risky
- **Transparency obowiÄ…zkowa** â€“ quality flags (INTERPOLATED, FILLED, MODEL_IMPUTED)
- **Conservative dla billing** (don't invent data dla money), aggressive dla internal analysis OK
- **&lt;2% imputation rate** target (wiÄ™cej â†’ fix root cause, nie mask problem)

**Demonstracja praktyczna**:
- Python code: Linear interpolation (pandas `interpolate()`)
- Comparison: Time series before/after imputation (wykres pokazuje filled gaps)
- InfluxDB: Query filtering imputed vs. measured

**MateriaÅ‚y pomocnicze**:
- Pandas documentation: `interpolate()`, `fillna()` methods
- Great Expectations: Missing data profiling
- Example imputationpolicy (YAML config)

**Typowe bÅ‚Ä™dy studenckie**:
- Imputacja bez flagowania (pretending it's measured) â€“ very bad!
- Forward-fill dla fast processes (power) â€“ leads to flat lines (unrealistic)
- Imputacja >15 min gaps â€“ model uncertainty too high

**Pytania studenckie**:
- Q: Czy ML (neural network) moÅ¼e robiÄ‡ lepszÄ… imputacjÄ™?
- A: TAK, ale: (1) complexity (training data, model maintenance), (2) explainability (auditors may not trust black box). Dla production: simple methods (linear, forward-fill) + transparency > complex ML.

- Q: Co jeÅ›li caÅ‚y dzieÅ„ brakuje (major outage)?
- A: NO imputation. Flag caÅ‚ego dnia jako MISSING, exclude z KPI. W monthly report: note ("1-day outage on 15 July, 0 production, equipment failure, repaired 16 July").

</InstructorNotes>

</Slide>

<VisualSeparator type="data" />

<Slide title="ðŸ“ˆ Dryft czujnikÃ³w (Sensor Drift Detection)" type="warning">

<KeyConcept title="Co to drift i dlaczego jest problem?">

**Drift (dryft)** = stopniowa zmiana charakterystyki czujnika w czasie (degradacja, aging, environmental exposure).

**Typy drift:**

**1. Zero drift (offset drift)**
- Zmiana punktu zerowego (baseline)
- **PrzykÅ‚ad**: Pyranometr pokazuje +5 W/mÂ² w nocy (powinno byÄ‡ 0) â†’ offset +5 W/mÂ²
- **WpÅ‚yw**: Systematyczne zawyÅ¼enie o 5 W/mÂ² â†’ error w PR (maÅ‚e G zwyÅ¼one %, duÅ¼e G zaniÅ¼one %)

**2. Span drift (gain drift)**
- Zmiana czuÅ‚oÅ›ci (sensitivity)
- **PrzykÅ‚ad**: Czujnik prÄ…du Hall po roku pokazuje 9.8 A gdy rzeczywiste 10.0 A â†’ gain drift -2%
- **WpÅ‚yw**: Wszystkie pomiary zaniÅ¼one o 2% â†’ underestimated production

**3. Nonlinearity drift**
- Zmiana krzywej charakterystyki (nieliniowoÅ›ci)
- **PrzykÅ‚ad**: NTC thermistor (nieliniowy) degraduje â†’ bÅ‚Ä…d roÅ›nie z temperaturÄ…
- **WpÅ‚yw**: Error varies z zakresem (maÅ‚e T: OK, wysokie T: duÅ¼y error)

---

### **Detekcja drift â€“ metody:**

**Metoda 1: Reference comparison (porÃ³wnanie do wzorca)**

MontaÅ¼ reference sensor (calibrated, known accuracy) obok testowanego:
- Daily/weekly snapshots (same conditions)
- Difference = drift

**PrzykÅ‚ad (pyranometry):**
```
Day 0 (installation):
  Sensor A (tested): 1000 W/mÂ²
  Sensor B (reference): 1000 W/mÂ²
  Difference: 0%

Day 180 (6 miesiÄ™cy):
  Sensor A: 970 W/mÂ²
  Sensor B: 1000 W/mÂ²
  Difference: -3% â†’ DRIFT detected!
```

**Action**: Recalibration sensor A (adjust sensitivity) lub wymiana

---

**Metoda 2: CUSUM (Cumulative Sum Control Chart)**

**Zasada**: Kumuluj rÃ³Å¼nice od baseline â†’ wykrywa stopniowe zmiany poziomu

$$
CUSUM_i = \sum_{j=1}^{i} (x_j - \mu_0 - k)
$$

Gdzie:
- x_j: Measurement
- Î¼â‚€: Baseline mean (z historical data, gdy czujnik byÅ‚ OK)
- k: Allowable drift (slack, typowo 0.5Ïƒ)

**Decision**: JeÅ›li |CUSUM| > threshold (typ. 4-5Ïƒ) â†’ alarm drift

**PrzykÅ‚ad (pyranometr drift detection):**
```python
import numpy as np

# Historical baseline (pierwsze 3 miesiÄ…ce po kalibracji)
baseline_mean = 850  # W/mÂ² (mean daily max)
baseline_std = 50    # W/mÂ²

# Current measurements (daily max, nastÄ™pne 6 miesiÄ™cy)
measurements = [845, 840, 842, 835, 830, 828, 825, 820, ...]  # Gradual decrease

k = 0.5 * baseline_std  # Slack = 25 W/mÂ²
threshold = 5 * baseline_std  # 250 W/mÂ²

cusum = 0
for i, x in enumerate(measurements):
    cusum += (x - baseline_mean - k)
    print(f"Day {i}: measurement={x}, CUSUM={cusum:.1f}")
    
    if abs(cusum) > threshold:
        print(f"DRIFT ALARM @ day {i}: CUSUM={cusum:.1f} > {threshold}")
        break

# Output (example):
# Day 0: measurement=845, CUSUM=-30.0
# Day 1: measurement=840, CUSUM=-65.0
# ...
# Day 12: measurement=820, CUSUM=-255.0 > 250 â†’ DRIFT ALARM! ðŸš¨
```

**Detection time**: 12 dni (po starcie drift) â†’ early warning, przed deterioration becomes severe

---

**Metoda 3: MAD (Median Absolute Deviation) â€“ outlier-resistant**

Dla sytuacji gdy mamy outliers (noise) ale teÅ¼ drift:

\[
MAD = median(|x_i - median(x)|)
\]

**Robust drift detection:**
```python
# Rolling window (30 days)
for window in rolling_30day_windows:
    median_current = np.median(window)
    mad_current = np.median(np.abs(window - median_current))
    
    # Compare to baseline
    if abs(median_current - baseline_median) > 3 * baseline_MAD:
        print(f"DRIFT detected: median shifted {median_current - baseline_median:.1f} W/mÂ²")
```

**Zalety MAD**: Odporny na outliers (pojedyncze spike nie trigger alarm)  
**Wady**: Wymaga wiÄ™cej data (30+ days window)

</KeyConcept>

<SupportingDetails title="ðŸ“Š Drift prevention i mitigation">

### **Prevention (zapobieganie):**

- **Regular calibration** (co 2-5 lat, zaleÅ¼nie od czujnika)
- **Controlled environment** (protect sensors od extreme temp, UV, corrosion)
- **Quality sensors** (First Class pyranometry, PT1000 Class A â€“ niÅ¼szy drift vs. cheap)
- **Redundancy** (2 sensors â†’ compare, detect drift early)

### **Mitigation (Å‚agodzenie skutkÃ³w):**

**Software correction** (jeÅ›li drift linear i known):
```python
# Apply correction factor (z porÃ³wnania do reference)
P_corrected = P_measured Ã— correction_factor
# correction_factor = 1.03 (jeÅ›li sensor drifted -3%, multiply by 1.03 to correct)
```

**Recalibration** (adjustment zera lub span):
- Zero: Offset w firmware/software
- Span: Multiply factor (gain adjustment)

**Replacement** (jeÅ›li drift nieodwracalny lub >5%):
- Typical: Elektrochemiczne czujniki Hâ‚‚S (1-3 lata), pyranometry (degradacja kopuÅ‚y >10 lat)

### **Monitoring drift (dashboard):**

Metrics do track:
- **Drift rate** (%/month): Trend sensor readingu relative to baseline
- **Time since calibration** (days): Alert @ 80% interwaÅ‚u (e.g., 580 days dla 2-year interval)
- **Comparison to peers**: Sensor A vs. sensors B/C (jeÅ›li A outlier â†’ suspect drift)

**Grafana alert:**
```
ALERT: Pyranometer PYR01 drift detected
- Current: 920 W/mÂ² (daily max avg, last 30 days)
- Baseline: 950 W/mÂ² (first 30 days post-calibration)
- Drift: -3.2% (-30 W/mÂ²)
- CUSUM: 265 W/mÂ²Â·days (threshold: 250)
â†’ ACTION: Schedule recalibration within 30 days
```

</SupportingDetails>

<Example title="Pyranometer drift â€“ detection i correction">

**Instalacja: Farma PV 10 MWp, 2 pyranometry POA (PYR01, PYR02)**

**Timeline:**

**Month 0 (styczeÅ„ 2023):** Instalacja i kalibracja obu
- PYR01: Kalibracja akredytowana, sensitivity 10.5 ÂµV/(W/mÂ²), U = Â±20 W/mÂ² (k=2)
- PYR02: Reference (Secondary Standard), sensitivity 8.2 ÂµV/(W/mÂ²), U = Â±10 W/mÂ²

**Month 6 (lipiec 2023):** Routine comparison
- Warunki: Clear sky, 12:00, stable
- PYR02 (reference): 1000 W/mÂ² (trusted)
- PYR01 (tested): **970 W/mÂ²**
- **Difference: -3%** (drift suspected)

**Verification (multiple days):**
```
July 10: PYR01=968, PYR02=1000 â†’ diff -3.2%
July 12: PYR01=972, PYR02=1005 â†’ diff -3.3%
July 15: PYR01=965, PYR02=995  â†’ diff -3.0%

Average drift: -3.2% (consistent, nie random noise â†’ DRIFT confirmed)
```

**CUSUM analysis (retroactively na last 6 months):**
```python
# Daily max G_POA (historical)
baseline_mean = 950  # W/mÂ² (Jan-Mar average, post-calibration)

# Actual daily max (Apr-Jul)
daily_max = [945, 940, 935, ..., 970]  # Gradual decline

# CUSUM calculation shows alarm triggered @ day 140 (mid-June)
# Detection lag: ~1 month (drift started Month 4, detected Month 6)
```

**Correction:**

**Option A: Software correction (immediate)**
```python
# Apply correction factor 1.031 (to compensate -3% drift)
G_POA_corrected = G_POA_measured Ã— 1.031

# Retroactive correction (re-calculate PR dla Apr-Jul)
# PR before correction: 80.1%
# PR after correction: 82.7% (+2.6 p.p.)
```

**Option B: Physical recalibration (permanent fix)**
- Send PYR01 to lab (koszt â‚¬500, czas 2 tygodnie)
- Alternative: In-situ outdoor comparison calibration (cheaper, â‚¬200, 3 dni)
- Result: New sensitivity = 10.2 ÂµV/(W/mÂ²) (was 10.5), adjusted

**Decision taken:** Option A (software) immediately + Option B (recalibration) scheduled (next planned maintenance, 2 miesiÄ…ce)

**Lessons:**
- **Redundant sensors** (PYR02 reference) kluczowy dla drift detection
- **Regular comparison** (co kwartaÅ‚) catches drift early (zamiast wait 2 years do next calibration)
- **Software correction** to temporary fix (permanent: recalibration)

</Example>

<InstructorNotes>

**Czas**: 16-18 min

**Przebieg**:
1. Drift definition i typy (3 min) â€“ zero, span, nonlinearity
2. Detekcja methods (6 min) â€“ reference comparison, CUSUM, MAD
3. Prevention i mitigation (3 min) â€“ calibration, correction, replacement
4. Case study (4 min) â€“ pyranometer drift, detection, correction
5. Q&A (2 min)

**Punkty kluczowe**:
- **Drift to inevitability** (all sensors drift over time), question is HOW FAST
- **CUSUM** to powerful tool (wykrywa stopniowe zmiany, ktÃ³re sÄ… invisible w daily noise)
- **Redundancy** (2 sensors different types/manufacturers) catches drift early
- **Software correction** = quick fix, recalibration = proper fix

**Demonstracja praktyczna**:
- Live CUSUM calculation (Python + matplotlib) â€“ pokazuje detection
- Comparison plot: PYR01 vs. PYR02 (6 months, drift visible as divergence)
- Grafana alert rule: CUSUM > threshold â†’ email notification

**MateriaÅ‚y pomocnicze**:
- NIST: "Drift in Measurement Systems" (technical note)
- CUSUM tutorial (statistical process control)
- Example drift monitoring dashboard (Grafana JSON)

**Typowe bÅ‚Ä™dy studenckie**:
- Ignorowanie drift ("czujnik dziaÅ‚a, wiÄ™c OK") â€“ drift moÅ¼e byÄ‡ 3-5% before visible
- Stosowanie tylko kalibracji (fixed intervals) bez continuous monitoring â€“ drift moÅ¼e occur between calibrations
- Brak redundancy (1 sensor) â†’ nie moÅ¼na detect drift (nothing to compare)

**Pytania studenckie**:
- Q: Czy kaÅ¼dy czujnik driftuje?
- A: TAK, ale z rÃ³Å¼nÄ… prÄ™dkoÅ›ciÄ…. Elektrochemiczne Hâ‚‚S: 5-15%/rok (fast). Pyranometry termopilowe: 1-3%/rok (slow). PT1000: &lt;0.5%/rok (bardzo slow). Depends on technology i environment.

- Q: Czy moÅ¼na prevent drift caÅ‚kowicie?
- A: NO (physics: degradacja materiaÅ‚Ã³w, thermal cycling, contamination). MoÅ¼na minimize (quality sensors, controlled environment), ale nie eliminate. Dlatego: drift monitoring + calibration schedule.

</InstructorNotes>

</Slide>

<VisualSeparator type="default" />

<Slide title="ðŸ“ Quiz: Walidacja, imputacja, drift" type="info">

<InteractiveQuiz 
  questions={[
    {
      question: "KtÃ³ra metoda walidacji wykryje problem: P_AC = 1500 kW, P_DC = 1400 kW (inverter efficiency >100%)?",
      options: [
        "Level 1 (Range check) â€“ P_AC w zakresie [0, P_rated]",
        "Level 2 (Physics check) â€“ P_AC > P_DC impossible",
        "Level 3 (Consistency check) â€“ cross-channel",
        "Wszystkie powyÅ¼sze"
      ],
      correctAnswer: 1,
      explanation: "P_AC > P_DC violates physics (efficiency >100%). Level 2 (physics check) wykryje. Level 1 moÅ¼e pass (jeÅ›li P_rated > 1500 kW). Level 3 to cross-channel (rÃ³Å¼ne sensor), ale tu to power balance (physics)."
    },
    {
      question: "Missing data gap: 5 min. Parametr: G_POA (irradiancja). KtÃ³ra metoda imputacji najbardziej appropriate?",
      options: [
        "Linear interpolation (smooth signal, gap short)",
        "Forward-fill (carry last value)",
        "NO imputation (gap too long)",
        "Model-based (physics model PV)"
      ],
      correctAnswer: 0,
      explanation: "5 min gap dla G_POA (relatywnie smooth w clear sky) â†’ linear interpolation reasonable. Forward-fill moÅ¼e miss changes (cloud transient). NO imputation to overkill (<15 min OK). Model-based possible ale overkill dla 5 min."
    },
    {
      question: "CUSUM alarm triggered (drift suspected). Co robisz NAJPIERW?",
      options: [
        "Natychmiastowa wymiana czujnika",
        "PorÃ³wnanie do reference sensor lub danych satelitarnych (verify drift is real)",
        "Ignorowanie (CUSUM moÅ¼e daÄ‡ false alarm)",
        "Software correction (apply factor immediately)"
      ],
      correctAnswer: 1,
      explanation: "Verify BEFORE action! CUSUM moÅ¼e false alarm (jeÅ›li baseline byÅ‚ wrong). PorÃ³wnanie do reference/satellite confirms drift. Opcja (a) wasteful (moÅ¼e nie byÄ‡ drift). Opcja (c) dangerous (moÅ¼e byÄ‡ real). Opcja (d) premature (verify first)."
    },
    {
      question: "Pyranometr driftowaÅ‚ -3% przez 6 miesiÄ™cy. Jaki wpÅ‚yw na PR calculation (bez korekty)?",
      options: [
        "PR zawyÅ¼ony ~3% (G zaniÅ¼ony â†’ E_expected niÅ¼szy â†’ PR wyÅ¼szy)",
        "PR zaniÅ¼ony ~3% (G niÅ¼szy â†’ produkcja niÅ¼sza)",
        "Brak wpÅ‚ywu (PR to ratio, drift cancels)",
        "Nie moÅ¼na oceniÄ‡ bez znajomoÅ›ci E_actual"
      ],
      correctAnswer: 0,
      explanation: "PR = E_actual / E_expected. E_expected = f(G_POA, ...). JeÅ›li G drifted -3% (zaniÅ¼ony), to E_expected -3% â†’ PR = E_actual / (E_expected Ã— 0.97) â†’ PR zawyÅ¼ony ~3%. Opcja (b) odwrotnie. Opcja (c) nieprawda (drift affects denominator). Opcja (d) nieprawda (direction of error is clear)."
    },
    {
      question: "Co to immutability principle w kontekÅ›cie imputacji?",
      options: [
        "Imputed data NIE moÅ¼e byÄ‡ used (only measured)",
        "Original data (with gaps) MUST byÄ‡ preserved (nie overwrite z imputed), imputed stored separately lub flagged",
        "Imputation musi byÄ‡ approved przez auditor",
        "Imputacja tylko dla CRITICAL data"
      ],
      correctAnswer: 1,
      explanation: "Immutability: Original data (gaps included) zachowane (append-only, audit trail). Imputed moÅ¼e byÄ‡ added (separate field lub quality flag), ale NIE overwrite original. Opcja (a) za strict (imputed can be used if flagged). Opcja (c) nieprawda (policy decision, nie per-imputation approval). Opcja (d) odwrotnie (imputation typically dla non-critical, CRITICAL = preserve gaps)."
    }
  ]}
/>

:::tip Rekomendacja po quizie
JeÅ›li uzyskaÅ‚eÅ› &lt;80% poprawnych odpowiedzi, przejrzyj sekcje o poziomach walidacji (physics checks kluczowe), metodach imputacji (kiedy ktÃ³rÄ…), i CUSUM drift detection. Zrozumienie data quality to fundament wiarygodnego monitoringu.
:::

</Slide>

</SlideContainer>

---

## Podsumowanie i wnioski

**Kluczowe punkty z tej sekcji:**

**1. 4 wymiary jakoÅ›ci danych**: Completeness (&gt;95%), Accuracy (Â±uncertainty), Consistency (physics + cross-channel), Timeliness (&lt;10 s).

**2. Walidacja 3-poziomowa**:
   - Level 1: Range checks (min/max) â†’ hard reject jeÅ›li fail
   - Level 2: Physics laws (P_AC â‰¤ P_DC, energy conservation) â†’ flag SUSPECT
   - Level 3: Consistency (âˆ‘I_string â‰ˆ I_DC, sensor correlation) â†’ flag SUSPECT
   - **Impact**: 3.6 p.p. rÃ³Å¼nica w PR (z/bez walidacji)

**3. Imputacja missing data**:
   - &lt;1 min: Linear interpolation (flag: INTERPOLATED)
   - 1-15 min: Forward-fill (slow) lub model-based (fast processes)
   - &gt;15 min: NO imputation (flag: MISSING, exclude z KPI)
   - **Transparency**: Quality flags obowiÄ…zkowe, dokumentuj policy

**4. Drift detection**:
   - Reference comparison (gold standard, wymaga 2nd sensor)
   - CUSUM (wykrywa stopniowe zmiany, early warning ~2 tygodnie-3 miesiÄ…ce)
   - MAD (outlier-resistant, dla noisy data)
   - **Typical drift**: Pyranometry 1-3%/rok, czujniki gazu 5-15%/rok

**5. Prevention &gt; Correction**: Quality sensors + regular calibration + redundancy &lt;&lt; cost bÅ‚Ä™dnych decisions (drift -3% â†’ PR error +3 p.p. â†’ potential disputes â‚¬10-50k).

**NastÄ™pne kroki:**
- Ä†wiczenie: Implementacja validation pipeline (Python + pandas)
- Lab: CUSUM drift detection (real sensor data)
- Przygotowanie do nastÄ™pnej sekcji: Windowing i resampling

---

**Dodatkowe zasoby:**
- **ISO 8000**: Data Quality framework
- **Great Expectations**: Python library dla data validation
- **CUSUM**: Statistical process control (SPC) literature
- **IEC 61724-1**: PV performance monitoring (data quality section)


