---
title: "Okna czasowe, resampling, downsampling"
sidebar_position: 2
---

import { 
  SlideContainer, 
  Slide, 
  KeyPoints, 
  SupportingDetails, 
  InstructorNotes,
  VisualSeparator,
  LearningObjective,
  KeyConcept,
  Example
} from '@site/src/components/SlideComponents';
import { InteractiveQuiz } from '@site/src/components/InteractiveQuiz';

<LearningObjective>
Po tej sekcji student potrafi:
- Dobraƒá odpowiedni rozmiar i typ okna czasowego (rectangular, Hanning, Hamming) dla r√≥≈ºnych proces√≥w (PV, wind, vibrations)
- Zastosowaƒá resampling i agregacje (mean, max, p95) do synchronizacji multi-rate data streams
- Zaprojektowaƒá kaskadƒô retencji (raw ‚Üí 1-min ‚Üí 15-min ‚Üí hourly) z optymalizacjƒÖ storage/query performance
- Oceniƒá trade-off miƒôdzy noise reduction a detection latency (window size vs. false positive rate)
</LearningObjective>

<SlideContainer>

<Slide title="ü™ü Okna czasowe (Windowing) ‚Äì wyg≈Çadzanie sygna≈Çu" type="info">

<KeyPoints title="üìã Dlaczego windowing? Problem: Noise vs. Signal">

**Raw data** zawiera:
- **Signal** (real process: PV power, wind speed, temperature)
- **Noise** (measurement uncertainty, EMI, quantization, transients)

**Bez filtrowania**: Noise triggers false alarms (FP) ‚Üí alarm fatigue  
**Z windowing**: Smooth signal ‚Üí reduced FP, ale **trade-off**: Detection latency ‚Üë

**Cel windowing**: Extract signal, suppress noise, preserve features (peaks, transients critical dla alarming).

</KeyPoints>

<KeyConcept title="Typy okien czasowych (Window Functions)">

**1. Rectangular Window (prostokƒÖtny, "box average")**

**Formula (moving average):**
\[
y_{smoothed}(t) = \frac{1}{N} \sum_{i=0}^{N-1} x(t - i)
\]

Gdzie:
- N: Window size (liczba pr√≥bek)
- x(t): Raw measurement

**Charakterystyka**:
- **Uniform weights** (ka≈ºda pr√≥bka r√≥wna waga 1/N)
- **Prostota** (easy to implement)
- **Wady**: Sharp cutoff ‚Üí spectral leakage (ringing artifacts w frequency domain)

**Zastosowanie**: KPI calculation (PR, energy totals), gdy nie potrzeba sharp filtering

**Przyk≈Çad (Python):**
```python
import pandas as pd

# Raw data (PV power, 1s interval)
df = pd.DataFrame({'P_AC': [1000, 1050, 1020, 1080, 1040, ...]})

# 60-second moving average (rectangular window)
df['P_AC_smooth'] = df['P_AC'].rolling(window=60, center=False).mean()

# Result: ka≈ºdy punkt = average ostatnich 60 s
```

---

**2. Hanning Window (weighted, smooth tapering)**

**Formula:**
\[
w(n) = 0.5 \left(1 - \cos\left(\frac{2\pi n}{N-1}\right)\right), \quad n=0,1,\ldots,N-1
\]

**Charakterystyka**:
- **Weights taper** (points na ko≈Ñcach okna majƒÖ mniejszƒÖ wagƒô ‚Üí smooth transition)
- **Better frequency response** (reduced spectral leakage vs. rectangular)
- **Application**: Signal processing (FFT analysis, vibrations, spectral analysis)

**Przyk≈Çad (scipy):**
```python
from scipy.signal import hann, convolve
import numpy as np

# Raw vibration data (accelerometer, 10 kHz)
signal = np.array([...])  # Noisy accel data

# Hanning window (101 samples, ~10 ms @ 10 kHz)
window = hann(101)
window /= window.sum()  # Normalize (sum = 1)

# Convolve (smoothing)
signal_smooth = convolve(signal, window, mode='same')
```

---

**3. Hamming Window (variant of Hanning, slightly different weights)**

**Formula:**
\[
w(n) = 0.54 - 0.46 \cos\left(\frac{2\pi n}{N-1}\right)
\]

**R√≥≈ºnica vs. Hanning**: Slightly higher sidelobes, but better mainlobe (trade-off)

**Application**: Similar do Hanning (spectral analysis), preferred w niekt√≥rych DSP contexts

---

**4. Exponential Weighted Moving Average (EWMA) ‚Äì adaptive**

**Formula:**
\[
y(t) = \alpha \cdot x(t) + (1-\alpha) \cdot y(t-1)
\]

Gdzie:
- Œ±: Smoothing factor (0 < Œ± < 1)
  - Œ± ‚Üí 1: More responsive (less smoothing)
  - Œ± ‚Üí 0: More smoothing (slower response)

**Charakterystyka**:
- **Adaptive** (recent data higher weight)
- **Single parameter** (Œ±)
- **Application**: Real-time trending (KPI dashboards, control loops)

**Przyk≈Çad (pandas):**
```python
# EWMA dla PV power (Œ± = 0.1 ‚Üí heavy smoothing)
df['P_AC_ewma'] = df['P_AC'].ewm(alpha=0.1, adjust=False).mean()
```

</KeyConcept>

<SupportingDetails title="üìä Wyb√≥r rozmiaru okna: Process dynamics">

**Kluczowa zasada**: **Window size ‚àù Process time constant**

### **Proces SLOW (PV power, temperature):**

- **Time constant**: œÑ ~ 1-10 min (clouds ‚Üí irradiance change ‚Üí power response)
- **Window size**: 1-5 min (60-300 s)
- **Rationale**: Noise high-frequency (1-10 Hz), signal low-frequency (<0.1 Hz) ‚Üí large window OK

**Przyk≈Çad: PV power smoothing**
```python
# Raw: 1 Hz (1000 samples/min) ‚Üí very noisy (inverter switching, measurement noise)
# Window: 60 s (60 samples) ‚Üí smooth, preserves cloud transients (œÑ ~ 2-5 min)

df['P_AC_1min'] = df['P_AC'].rolling(window=60).mean()
```

**Impact:**
- **Noise reduction**: œÉ (std. dev) reduced by ‚àöN = ‚àö60 ‚âà **7.7√ó** (Central Limit Theorem)
- **Detection latency**: +30 s (half window, if center=True) lub +60 s (full window, center=False)

---

### **Proces FAST (wind gusts, vibrations):**

- **Time constant**: œÑ ~ 0.1-1 s (vibrations: bearing defects 10-100 Hz)
- **Window size**: Depends on application:
  - **Trending/KPI**: 10 s - 1 min (reduce noise dla dashboard)
  - **Event detection**: NO smoothing lub very small window (preserve transients!)

**Przyk≈Çad: Wind speed (10 Hz sampling, gust detection)**
```python
# Raw: 10 Hz (100 samples/s) ‚Üí dla KPI (10-min average)
df['Wind_10min'] = df['Wind'].rolling(window=6000).mean()  # 10 min √ó 60 s √ó 10 Hz

# Dla gust alarm: NO smoothing (detect peaks immediately)
df['Gust_alarm'] = df['Wind'] > 25  # m/s threshold, raw data
```

---

### **Trade-off: Window size vs. Detection latency**

**Tabela:**

| Window Size | Noise Reduction | Detection Latency | False Positive Rate | Application |
|-------------|-----------------|-------------------|---------------------|-------------|
| **Small** (10 s) | ‚àö10 ‚âà 3√ó | +5 s (half window) | HIGH (noise passes) | Fast process, critical alarms |
| **Medium** (60 s) | ‚àö60 ‚âà 8√ó | +30 s | MEDIUM | Balance (PV, most OZE) |
| **Large** (300 s, 5 min) | ‚àö300 ‚âà 17√ó | +2.5 min | LOW | KPI, trending (nie alarms) |

**Decision guideline:**
- **Alarms (critical)**: Small window (accept higher FP, fast detection)
- **KPI (reporting)**: Large window (smooth, stable)
- **Control loops**: Medium (balance responsiveness/noise)

</SupportingDetails>

<Example title="Case study: PV farm alarm ‚Äì window size optimization">

**Problem**: Farma PV 10 MWp, alarm "Low PR" (PR < 75%) triggering **50√ó / day** (false positives, alarm fatigue).

**Root cause analysis:**
1. PR calculated every 1 s (real-time)
2. G_POA noise: œÉ = ¬±15 W/m¬≤ (¬±1.5% @ 1000 W/m¬≤)
3. P_AC noise: œÉ = ¬±50 kW (¬±0.5% @ 10 MW)
4. Combined: PR noise œÉ ‚âà ¬±2% ‚Üí PR fluctuates 80% ¬± 2% ‚Üí frequent dips <75%

**Solution options:**

**Option A: Increase alarm threshold (75% ‚Üí 70%)**
- **Pros**: Fewer FP (threshold below noise floor)
- **Cons**: Delayed detection (real problem @ 72% nie trigger alarm)
- **Result**: NOT chosen (trade-off too severe)

**Option B: Windowing (smooth PR before alarming)**
- **Implementation**: 5-min moving average dla PR calculation
```python
# Before (1 s raw)
PR_raw = E_actual / E_expected  # Every 1 s

# After (5-min window)
PR_5min = df[['E_actual', 'E_expected']].rolling(window=300).sum()
PR_5min = PR_5min['E_actual'] / PR_5min['E_expected']
```

**Impact:**
- **Noise reduction**: ‚àö300 ‚âà **17√ó** ‚Üí œÉ_PR: ¬±2% ‚Üí ¬±0.12% (poni≈ºej threshold)
- **FP rate**: 50/day ‚Üí **2/day** (-96%! Huge improvement)
- **Detection latency**: +2.5 min (acceptable dla non-critical PR monitoring)

**Final tuning:**
- Alarm threshold: 75% (unchanged, proper level)
- Window: 5 min dla alarming, 1 s preserved dla trending (dashboard shows both)
- **Additional rule**: Alarm ONLY if PR <75% persists for **2 consecutive windows** (10 min total) ‚Üí further reduces transient FP

**Result (post-deployment, 1 month):**
- FP rate: **0.5/day** (1 w 2 dni, mostly legitimate borderline cases)
- True positive detection: 100% (all real PR drops detected within 12 min, acceptable)
- Operator satisfaction: ‚Üë‚Üë (alarm credibility restored)

</Example>

<InstructorNotes>

**Czas**: 16-18 min

**Przebieg**:
1. Dlaczego windowing (4 min) ‚Äì noise vs. signal, false positives
2. Typy okien (5 min) ‚Äì rectangular, Hanning, Hamming, EWMA (formulas + examples)
3. Wyb√≥r rozmiaru okna (4 min) ‚Äì process dynamics (slow PV vs. fast vibrations), table trade-offs
4. Case study (4 min) ‚Äì PV alarm optimization (96% FP reduction)
5. Q&A (2 min)

**Punkty kluczowe**:
- **"Window size = 3-5√ó process time constant"** rule of thumb
- **Noise reduction ‚àù ‚àöN** (Central Limit Theorem) ‚Äì du≈ºe okno = dramatyczna poprawa SNR
- **Trade-off**: Smoothing (FP reduction) vs. Latency (detection delay)
- **Separate windows** dla alarming (small) vs. KPI (large) ‚Äì different objectives!

**Demonstracja praktyczna**:
- Live Python: Raw vs. smoothed signal (matplotlib plot)
- Comparison: 10 s vs. 60 s vs. 300 s window (pokazuje trade-off)
- Hanning vs. rectangular (frequency domain plot ‚Äì pokazuje spectral leakage)

**Materia≈Çy pomocnicze**:
- Scipy signal processing documentation (`rolling`, `ewm`, `convolve`)
- DSP textbook: Window functions chapter (Harris, 1978 classic paper)
- Example code: Alarm logic z windowing (Python)

**Typowe b≈Çƒôdy studenckie**:
- "Bigger window = always better" ‚Äì NO! Zale≈ºy od application (alarms vs. KPI)
- Applying large window do fast process (vibrations) ‚Üí miss transients (defeats purpose)
- Nie rozr√≥≈ºnianie noise (measurement) od signal variability (real clouds) ‚Äì window removes obie, ale to intended

**Pytania studenckie**:
- Q: Dlaczego Hanning zamiast rectangular?
- A: Dla **spectral analysis** (FFT): Hanning reduces spectral leakage (smooth taper ‚Üí less ringing). Dla **simple averaging** (KPI): rectangular OK (prostszy, equivalent dla time-domain smoothing).

- Q: Jak dobraƒá Œ± w EWMA?
- A: Empirically lub z process œÑ: Œ± ‚âà 2/(N+1), gdzie N = equivalent window size. For œÑ=60 s, Œ± ‚âà 0.03 (heavy smoothing). Tune based on desired responsiveness.

</InstructorNotes>

</Slide>

<VisualSeparator type="technical" />

<Slide title="‚è±Ô∏è Resampling i synchronizacja ‚Äì multi-rate data streams" type="tip">

<KeyConcept title="Problem: Niejednorodne czƒôstotliwo≈õci pr√≥bkowania">

**Real-world OZE system** ma **multiple data streams** z r√≥≈ºnymi sampling rates:

| Stream | Rate | Example |
|--------|------|---------|
| **High-frequency** (HF) | 1-100 Hz | Vibrations (10 kHz), electrical waveforms (1 kHz), wind gusts (10 Hz) |
| **Medium** | 1 Hz - 1/min | PV power (1 Hz), wind speed average (1 Hz), temperatures (10 s) |
| **Low-frequency** (LF) | 1/min - 1/hour | Energy totals (15 min), KPI (hourly), weather forecasts (hourly) |
| **Event-driven** | Irregular | Alarms, SOE, state changes (trigger on event, nie periodic) |

**Challenge**: Jak analyze/correlate streams z r√≥≈ºnymi rates? **Solution**: **Resampling** ‚Üí common time grid.

</KeyConcept>

<SupportingDetails title="üîß Resampling methods: Upsampling vs. Downsampling">

### **Downsampling (decimation): High rate ‚Üí Low rate**

**Proces**: Reduce sampling rate (e.g., 1 Hz ‚Üí 1/min)

**Method**: **Aggregation** (mean, max, min, sum, p95, ...)

**Example (PV power: 1 Hz ‚Üí 1 min):**
```python
# Raw data (1 Hz, 3600 samples/hour)
df_1Hz = pd.read_csv('pv_power_1Hz.csv', parse_dates=['timestamp'])
df_1Hz.set_index('timestamp', inplace=True)

# Downsample: 1-min intervals (mean aggregation)
df_1min = df_1Hz.resample('1T').agg({
    'P_AC': 'mean',       # Average power
    'I_DC': 'mean',       # Average current
    'U_DC': 'mean',       # Average voltage
    'E_total': 'last',    # Energy counter (cumulative, take last value)
    'Faults': 'sum'       # Count faults in interval
})

# Result: 60 samples/hour (60√ó reduction)
```

**Aggregation functions:**

| Function | Use Case | Formula | Example |
|----------|----------|---------|---------|
| **mean** | Average (power, voltage, current, temps) | \( \bar{x} = \frac{1}{N}\sum x_i \) | P_AC_1min = mean(P_AC_1Hz, 60 samples) |
| **max** | Peak values (Gmax dla irradiance, wind gusts) | \( x_{max} = \max(x_i) \) | G_max_15min = max(G_POA_1Hz) |
| **min** | Minimum (SoC_min, voltage sags) | \( x_{min} = \min(x_i) \) | U_min_1min = min(U_grid_1Hz) |
| **sum** | Totals (energy, fault counts, event counts) | \( S = \sum x_i \) | E_15min = sum(P_AC_1Hz) √ó Œît |
| **p95** (percentile 95%) | Robust max (excludes outliers) | 95th percentile | P95_power = quantile(P_AC, 0.95) |
| **last** | Cumulative counters (energy total, state) | Last value in interval | E_total_end = last(E_total) |

**Decision guide:**
- **Mean**: General default (stable, robust)
- **Max**: Extreme values (safety, compliance)
- **Sum**: Counters/totals (energy billing)
- **p95**: Robust max (when outliers expected)

---

### **Upsampling (interpolation): Low rate ‚Üí High rate**

**Proces**: Increase sampling rate (e.g., 1/min ‚Üí 1 Hz)

**Method**: **Interpolation** (forward-fill, linear, polynomial, cubic spline)

**Example (temperature: 1/min ‚Üí 1 Hz):**
```python
# Low-rate data (1 sample/min)
df_1min = pd.DataFrame({
    'timestamp': pd.date_range('2024-01-01 10:00', periods=10, freq='1T'),
    'T_amb': [22.5, 22.6, 22.7, 22.8, ...]
}).set_index('timestamp')

# Upsample: 1 Hz (create empty slots)
df_1Hz = df_1min.resample('1S').asfreq()

# Fill gaps: forward-fill (carry last value)
df_1Hz['T_amb'] = df_1Hz['T_amb'].fillna(method='ffill')

# Alternative: linear interpolation
df_1Hz['T_amb'] = df_1Hz['T_amb'].interpolate(method='linear')
```

**When to upsample?**
- Aligning multi-rate streams (common time grid dla correlation/analysis)
- Visualization (smooth plot)

**Caution**: Upsampling NIE adds information (interpolated data to estimates, nie real measurements). Mark jako synthetic (quality flag).

---

### **Synchronizacja time grids (alignment):**

**Problem**: Streams z r√≥≈ºnymi timestamps (jitter, clock skew, different sources).

**Solution**: **Resample do common grid** (e.g., 1-min intervals on :00 seconds).

**Example (align PV inverters 1-10, all 1 Hz but timestamps differ):**
```python
# Inverter 1: timestamps @ 10:00:00.123, 10:00:01.127, ... (jitter ¬±200 ms)
# Inverter 2: timestamps @ 10:00:00.456, 10:00:01.451, ...
# ...

# Solution: Resample ALL to 1 Hz exact grid (:00.000 seconds)
common_grid = pd.date_range('2024-01-01 10:00:00', periods=3600, freq='1S')

df_inv1_aligned = df_inv1.reindex(common_grid, method='nearest', tolerance='500ms')
df_inv2_aligned = df_inv2.reindex(common_grid, method='nearest', tolerance='500ms')
# ...

# Now: All inverters on SAME timestamps ‚Üí can sum/aggregate easily
df_farm = pd.concat([df_inv1_aligned['P_AC'], df_inv2_aligned['P_AC'], ...], axis=1).sum(axis=1)
```

**Tolerance**: Allow matching within ¬±500 ms (if no sample within tolerance ‚Üí NaN).

</SupportingDetails>

<Example title="Multi-rate correlation: Wind farm fault diagnosis">

**Scenario**: Wind turbine #5 (3 MW), suspected gearbox issue.

**Data streams:**

| Stream | Rate | Source |
|--------|------|--------|
| **Vibration** (accelerometer) | 10 kHz | CMS (Condition Monitoring System) |
| **Power, RPM, temps** | 1 Hz | SCADA |
| **Alarms** | Event-driven | Turbine controller |
| **Wind speed** (nacelle) | 1 Hz | Anemometer |

**Analysis goal**: Correlate vibration spikes z power/RPM changes ‚Üí diagnose fault.

**Step 1: Downsample vibrations (10 kHz ‚Üí 1 Hz)**
```python
# Vibration raw: 10 kHz ‚Üí very high volume (36 GB/hour!)
# Downsample: RMS (root-mean-square) every 1 s
df_vib_1Hz = df_vib_10kHz.resample('1S').apply(lambda x: np.sqrt(np.mean(x**2)))

# Result: 3600 samples/hour (10000√ó reduction, storage-friendly)
```

**Step 2: Align all streams @ 1 Hz grid**
```python
# Common grid (1 Hz, 1 hour window)
grid = pd.date_range('2024-07-15 14:00:00', periods=3600, freq='1S')

# Resample each
df_vib = df_vib_1Hz.reindex(grid, method='nearest', tolerance='1S')
df_power = df_power_raw.reindex(grid, method='nearest')
df_rpm = df_rpm_raw.reindex(grid, method='nearest')
df_wind = df_wind_raw.reindex(grid, method='nearest')

# Merge
df = pd.concat([df_vib, df_power, df_rpm, df_wind], axis=1)
```

**Step 3: Correlation analysis**
```python
# Correlation matrix
corr_matrix = df.corr()
print(corr_matrix)

# Result:
#            Vibration  Power   RPM    Wind
# Vibration     1.00   -0.68  -0.72   0.15
# Power        -0.68    1.00   0.98   0.85
# RPM          -0.72    0.98   1.00   0.82
# Wind          0.15    0.85   0.82   1.00
```

**Interpretation:**
- **Vibration vs. Power**: r = -0.68 (negative correlation!) ‚Üí High vibration gdy LOW power
- **Vibration vs. RPM**: r = -0.72 (stronger negative) ‚Üí High vibration @ LOW RPM
- **Insight**: Gearbox fault occurs @ low-speed, high-torque conditions (startup/shutdown ramps)

**Step 4: Time-series plot (aligned)**
```python
import matplotlib.pyplot as plt

fig, ax = plt.subplots(3, 1, figsize=(12, 8), sharex=True)

ax[0].plot(df.index, df['Power'], label='Power (kW)')
ax[0].set_ylabel('Power [kW]')
ax[0].legend()

ax[1].plot(df.index, df['RPM'], label='RPM', color='orange')
ax[1].set_ylabel('RPM')
ax[1].legend()

ax[2].plot(df.index, df['Vibration'], label='Vibration RMS [g]', color='red')
ax[2].set_ylabel('Vibration [g]')
ax[2].axhline(y=0.5, color='red', linestyle='--', label='Alarm threshold')
ax[2].legend()
ax[2].set_xlabel('Time')

plt.tight_layout()
plt.show()
```

**Finding**: Vibration spikes @ 14:12, 14:28, 14:45 ‚Üí correlate z RPM ramps (acceleration phase) ‚Üí gearbox bearing fault confirmed.

**Action**: Scheduled maintenance (bearing replacement), prevented catastrophic failure (cost savings ‚Ç¨200k).

</Example>

<InstructorNotes>

**Czas**: 14-16 min

**Przebieg**:
1. Multi-rate problem (3 min) ‚Äì table r√≥≈ºnych rates w OZE
2. Downsampling (4 min) ‚Äì aggregation methods (mean, max, sum, p95)
3. Upsampling (2 min) ‚Äì interpolation (forward-fill, linear)
4. Alignment (2 min) ‚Äì common time grid, jitter handling
5. Case study (4 min) ‚Äì wind turbine fault diagnosis, correlation

**Punkty kluczowe**:
- **Resampling = foundational** dla multi-source data analysis
- **Aggregation function matters** (mean ‚â† max ‚â† sum) ‚Äì zale≈ºy od physical meaning
- **Upsampling nie dodaje info** (synthetic data, flag accordingly)
- **Time alignment kluczowy** dla correlation (clock sync NTP/PTP!)

**Demonstracja praktyczna**:
- Pandas `resample()` demo (live Jupyter)
- Pokazuje different aggregations (mean vs. max) ‚Üí different insights
- Alignment visualization (before/after: misaligned ‚Üí aligned timestamps)

**Materia≈Çy pomocnicze**:
- Pandas time series documentation (`resample`, `reindex`, `asfreq`)
- Example notebook: Multi-rate OZE data processing
- NTP/PTP time sync best practices (link back to Wyk≈Çad 02)

**Typowe b≈Çƒôdy studenckie**:
- Upsampling thinking it adds resolution ‚Äì NO, only interpolates (pretend)
- Nie specifying aggregation function (pandas default: mean, ale mo≈ºe nie be appropriate)
- Ignoring timezone i DST (resample @ DST transitions ‚Üí gaps/overlaps)

**Pytania studenckie**:
- Q: Co z event-driven data (alarms, SOE)? Jak resample?
- A: Event-driven to discrete (nie continuous time series). Options: (1) Count events per interval (resample + count), (2) Preserve as-is (timestamp + event, merge z time-series based on timestamp lookup).

- Q: Kt√≥ry aggregation dla energy billing (15-min intervals)?
- A: **sum(Power √ó Œît)** dla energy (Wh), lub **mean(Power) √ó interval_duration** (equivalent). Verify with meter reading (ground truth).

</InstructorNotes>

</Slide>

<VisualSeparator type="data" />

<Slide title="üóúÔ∏è Downsampling cascade i retencja ‚Äì storage optimization" type="warning">

<KeyConcept title="Problem: Infinite storage nie istnieje">

**Typical OZE farm (10 MW PV):**
- **Sensors**: 500 data points (inverters, strings, weather, grid)
- **Sampling**: 1 Hz (1 sample/s per point)
- **Data rate**: 500 points √ó 1 Hz √ó 8 bytes (float64) = **4 kB/s** = 345 MB/day = **126 GB/year** (raw!)

**For 20-year lifetime**: 126 GB √ó 20 = **2.5 TB** (per farm!) + compliance wymaga 10+ years retention.

**Solution**: **Downsampling cascade** + **Retention policies** (progressive aggregation + deletion)

</KeyConcept>

<SupportingDetails title="üéØ Retention cascade design ‚Äì 4 tiers">

**Tier 1: RAW (high-resolution, short-term)**
- **Rate**: 1 Hz (original sampling)
- **Retention**: **7 days** (168 hours)
- **Purpose**: Detailed diagnostics, event investigation (recent)
- **Storage**: 345 MB/day √ó 7 = **2.4 GB** (rolling window)

**Tier 2: 1-MINUTE aggregated**
- **Rate**: 1/min (60√ó reduction)
- **Aggregation**: mean (dla most), max (dla peaks), min (dla sags)
- **Retention**: **6 months** (180 days)
- **Purpose**: KPI analysis, weekly/monthly reports, trend analysis
- **Storage**: 2.4 GB / 60 √ó 180 = **7.2 GB**

**Tier 3: 15-MINUTE aggregated**
- **Rate**: 1/15min (900√ó reduction vs. raw)
- **Aggregation**: mean (standard dla grid telemetry OSD)
- **Retention**: **3 years** (compliance, contractual SLA)
- **Purpose**: Billing, regulatory reporting, long-term performance
- **Storage**: 2.4 GB / 900 √ó 1095 days = **2.9 GB**

**Tier 4: HOURLY / DAILY aggregated**
- **Rate**: 1/hour lub 1/day
- **Aggregation**: sum (energy totals), mean (PR, CF)
- **Retention**: **10-20 years** (archival, historical analysis)
- **Purpose**: Lifetime performance, degradation analysis, ROI
- **Storage**: Negligible (~100 MB dla 20 years)

---

### **Total storage (4-tier cascade):**

| Tier | Retention | Storage | % of Total |
|------|-----------|---------|------------|
| Raw (1 Hz) | 7 days | 2.4 GB | 19% |
| 1-min | 6 months | 7.2 GB | 57% |
| 15-min | 3 years | 2.9 GB | 23% |
| Hourly | 20 years | 0.1 GB | 1% |
| **TOTAL** | ‚Äî | **12.6 GB** | 100% |

**Comparison**: Bez cascade (20 years raw @ 1 Hz): **2.5 TB**  
**Reduction**: 2500 GB ‚Üí 12.6 GB = **200√ó** savings! üéâ

</SupportingDetails>

<Example title="InfluxDB retention policy ‚Äì implementation">

**Platform**: InfluxDB 2.x (time-series database)

**Configuration (Flux / CLI):**

```sql
-- Tier 1: RAW (7 days retention)
CREATE RETENTION POLICY "raw_7d" ON "solar_farm"
  DURATION 7d           -- Auto-delete data older than 7 days
  REPLICATION 1         -- Single replica (edge deployment)
  DEFAULT;              -- Default policy (new writes go here)

-- Tier 2: 1-MIN aggregation (6 months retention)
CREATE RETENTION POLICY "aggregated_1min_6mo" ON "solar_farm"
  DURATION 180d         -- 6 months
  REPLICATION 1;

-- Continuous Query (CQ): Auto-aggregate raw ‚Üí 1-min every hour
CREATE CONTINUOUS QUERY "cq_downsample_1min" ON "solar_farm"
BEGIN
  SELECT 
    mean("P_AC") AS "P_AC_mean",
    max("G_POA") AS "G_POA_max",
    min("U_DC") AS "U_DC_min",
    sum("Faults") AS "Fault_count"
  INTO "solar_farm"."aggregated_1min_6mo".:MEASUREMENT
  FROM "solar_farm"."raw_7d"./.*/ 
  GROUP BY time(1m), *    -- 1-minute buckets, preserve all tags
END;

-- Tier 3: 15-MIN aggregation (3 years)
CREATE RETENTION POLICY "aggregated_15min_3y" ON "solar_farm"
  DURATION 1095d;       -- 3 years

CREATE CONTINUOUS QUERY "cq_downsample_15min" ON "solar_farm"
BEGIN
  SELECT 
    mean("P_AC_mean") AS "P_AC",
    mean("G_POA_max") AS "G_POA"
  INTO "solar_farm"."aggregated_15min_3y".:MEASUREMENT
  FROM "solar_farm"."aggregated_1min_6mo"./.*/ 
  GROUP BY time(15m), *
END;

-- Tier 4: HOURLY aggregation (20 years)
CREATE RETENTION POLICY "aggregated_hourly_20y" ON "solar_farm"
  DURATION 7300d;       -- 20 years

CREATE CONTINUOUS QUERY "cq_downsample_hourly" ON "solar_farm"
BEGIN
  SELECT 
    sum("P_AC") * 15 / 60 AS "E_kWh",    -- Energy (kWh) from 15-min power (kW)
    mean("P_AC") AS "P_avg"
  INTO "solar_farm"."aggregated_hourly_20y".:MEASUREMENT
  FROM "solar_farm"."aggregated_15min_3y"./.*/ 
  GROUP BY time(1h), *
END;
```

**Automation**: Continuous Queries (CQ) run automatically (every hour) ‚Üí progressive downsampling, old data auto-deleted per retention policy.

**Benefit**: **Set-and-forget** (no manual intervention, storage self-manages).

---

### **Query strategy (depends on time range):**

```python
# Python client (query optimization)
def query_power(start, end, farm_id):
    duration = (end - start).total_seconds()
    
    if duration <= 7 * 86400:  # ‚â§ 7 days ‚Üí use RAW
        bucket = "raw_7d"
        interval = "1s"
    elif duration <= 180 * 86400:  # ‚â§ 6 months ‚Üí use 1-MIN
        bucket = "aggregated_1min_6mo"
        interval = "1m"
    elif duration <= 1095 * 86400:  # ‚â§ 3 years ‚Üí use 15-MIN
        bucket = "aggregated_15min_3y"
        interval = "15m"
    else:  # > 3 years ‚Üí use HOURLY
        bucket = "aggregated_hourly_20y"
        interval = "1h"
    
    # Flux query
    query = f'''
    from(bucket: "{bucket}")
      |> range(start: {start}, stop: {end})
      |> filter(fn: (r) => r.farm_id == "{farm_id}")
      |> filter(fn: (r) => r._measurement == "power")
      |> aggregateWindow(every: {interval}, fn: mean)
    '''
    return influx_client.query(query)
```

**Optimization**: Automatic tier selection based on query range ‚Üí fast queries (minimal data scanned).

</Example>

<Example title="Trade-off analysis: PR calculation accuracy vs. storage">

**Question**: Jaki minimal aggregation interval dla PR accuracy?

**Test setup:**
- Farma PV 10 MWp, 1 month data (Jan 2024)
- PR baseline (ground truth): **82.5%** (calculated z 1 Hz raw data)

**Test aggregation levels:**

| Aggregation | Interval | PR calculated | Œî vs. baseline | Storage (1 month) | Comment |
|-------------|----------|---------------|----------------|-------------------|---------|
| **Raw** | 1 Hz | 82.50% | 0.00% (reference) | 10.3 GB | Baseline, high storage |
| **1-MIN** | 60 s | 82.49% | -0.01% | 172 MB | Minimal error, 60√ó reduction ‚úÖ |
| **15-MIN** | 900 s | 82.47% | -0.03% | 11.5 MB | Acceptable error, 900√ó reduction ‚úÖ |
| **HOURLY** | 3600 s | 82.38% | -0.12% | 2.9 MB | Noticeable bias (smooths cloud events) ‚ö†Ô∏è |
| **DAILY** | 86400 s | 81.95% | -0.55% | 122 kB | Significant error (too coarse) ‚ùå |

**Findings:**
- **1-MIN aggregation**: Virtually identical to raw (<0.01% error) ‚Üí **RECOMMENDED minimum dla KPI**
- **15-MIN**: Standard dla grid telemetry (OSD compliance), error <0.05% ‚Üí **ACCEPTABLE dla billing/reporting**
- **HOURLY**: Smoothing introduces bias (-0.12%) ‚Üí OK dla trending, **NOT for billing**
- **DAILY**: Too coarse (-0.55% error, misses intra-day variability) ‚Üí **Archival only**

**Recommendation**: **15-min minimum** dla long-term retention (3 years), **1-min dla KPI analysis** (6 months), raw (7 days emergency diagnostics).

</Example>

<InstructorNotes>

**Czas**: 16-18 min

**Przebieg**:
1. Storage problem (3 min) ‚Äì math shows 2.5 TB dla 20 years (impractical)
2. Retention cascade design (5 min) ‚Äì 4 tiers (raw, 1-min, 15-min, hourly)
3. InfluxDB implementation (4 min) ‚Äì retention policies + continuous queries
4. Trade-off analysis (3 min) ‚Äì PR accuracy vs. aggregation interval
5. Q&A (2 min)

**Punkty kluczowe**:
- **Cascade downsampling** = mandatory (nie optional) dla long-term sustainability
- **200√ó storage reduction** (2.5 TB ‚Üí 12 GB) bez significant loss accuracy
- **15-min standard** dla compliance (grid telemetry, billing)
- **Automation** (CQ / scheduled tasks) ‚Üí no manual intervention

**Demonstracja praktyczna**:
- InfluxDB dashboard: Show 4 retention policies (data volume per tier)
- Query comparison: Raw vs. 1-min vs. 15-min (speed difference, accuracy)
- Example: Grafana query auto-selects tier based on time range (transparent to user)

**Materia≈Çy pomocnicze**:
- InfluxDB documentation: Retention policies, Continuous Queries
- Example configuration files (retention policies dla different farm sizes)
- Cost analysis: Storage costs (cloud S3/Azure) vs. on-prem

**Typowe b≈Çƒôdy studenckie**:
- "Store everything raw forever" ‚Äì impossible (storage/cost)
- Aggregating before preservation (irreversible) ‚Äì cascade allows re-aggregation later
- Nie testing PR accuracy before deploying retention ‚Üí discover error too late

**Pytania studenckie**:
- Q: Co je≈õli discover error AFTER raw data deleted (>7 days)?
- A: CANNOT recover. Dlatego: (1) Extend raw retention dla critical commissioning period (30 days), (2) Verify 1-min aggregation accuracy BEFORE deleting raw, (3) Backup critical periods (e.g., acceptance tests).

- Q: Czy mo≈ºna mieƒá r√≥≈ºne retention dla r√≥≈ºnych measurements (power vs. temps)?
- A: TAK! Separate retention policies per measurement type (power: 7 days raw, temps: 1 day raw ‚Äì less critical). Flexible design.

</InstructorNotes>

</Slide>

<VisualSeparator type="default" />

<Slide title="üìù Quiz: Windowing, resampling, retention" type="info">

<InteractiveQuiz 
  questions={[
    {
      question: "Window size: 60 s, sampling 1 Hz. Jaka noise reduction (theoretical, Central Limit Theorem)?",
      options: [
        "‚àö60 ‚âà 7.7√ó reduction in œÉ (std. dev)",
        "60√ó reduction (linear z window size)",
        "1/60 = 0.0167√ó (60√ó smaller)",
        "Zale≈ºy od noise type (nie mo≈ºna okre≈õliƒá bez spektrum)"
      ],
      correctAnswer: 0,
      explanation: "Central Limit Theorem: œÉ_avg = œÉ_raw / ‚àöN. Dla N=60, reduction = ‚àö60 ‚âà 7.7√ó. Opcja (b) nieprawda (linear only dla sum, nie average). Opcja (c) odwrotnie. Opcja (d) partially true (CLT assumes independent noise), ale ‚àöN to good approximation dla most cases."
    },
    {
      question: "False Positive rate: 50/day. Po zastosowaniu 5-min windowing: 2/day (-96%). Detection latency wzr√≥s≈Ç o?",
      options: [
        "+2.5 min (half window, center=False)",
        "+5 min (full window)",
        "+10 min (2√ó window dla confirmation)",
        "Brak latency (windowing nie op√≥≈∫nia)"
      ],
      correctAnswer: 0,
      explanation: "Windowing (center=False, default): Detection latency = half window size (signal musi 'fill' window). 5 min / 2 = +2.5 min. Opcja (b) full window tylko je≈õli wymaga ENTIRE window above threshold (rare). Opcja (c) to additional confirmation rule (nie windowing itself). Opcja (d) nieprawda (windowing ALWAYS adds latency)."
    },
    {
      question: "Downsampling 1 Hz ‚Üí 15 min: Kt√≥ra aggregacja dla G_POA (irradiancja) dla PR calculation?",
      options: [
        "mean (average irradiance over 15 min)",
        "max (peak irradiance)",
        "min (lowest irradiance)",
        "sum (total irradiance)"
      ],
      correctAnswer: 0,
      explanation: "PR = E_actual / E_expected, gdzie E_expected ‚àù G_average (nie peak). Mean to correct aggregation (represents average energy available). Max/min to extremes (nie representative). Sum would need unit conversion (W/m¬≤ ‚Üí Wh/m¬≤, requires √ó Œît)."
    },
    {
      question: "Retention cascade: raw 7d, 1-min 6mo, 15-min 3y. Query dla last 2 months KPI analysis ‚Äì kt√≥ry tier?",
      options: [
        "1-min (2 months < 6 months retention, sufficient resolution)",
        "Raw (highest resolution always best)",
        "15-min (standard dla KPI)",
        "Hourly (faster query)"
      ],
      correctAnswer: 0,
      explanation: "2 months < 6 months ‚Üí 1-min tier available i sufficient (0.01% error vs. raw, 60√ó faster query). Opcja (b) raw may nie byƒá available (>7 days deleted) i overkill (slow query). Opcja (c) 15-min OK ale lower resolution (1-min better je≈õli available). Opcja (d) hourly too coarse dla KPI."
    },
    {
      question: "PR error: Raw (82.50%) vs. Hourly aggregation (82.38%, -0.12%). Dlaczego bias?",
      options: [
        "Hourly smoothing misses intra-hour variability (cloud transients averaged out, denominator E_expected overestimated)",
        "Rounding errors (float precision loss)",
        "Timezone issues (DST transitions)",
        "Measurement drift (sensor degradation over month)"
      ],
      correctAnswer: 0,
      explanation: "Hourly aggregation smooths cloud events ‚Üí G_average slightly higher (transients averaged) ‚Üí E_expected overestimated ‚Üí PR underestimated. Opcja (b) negligible (float64 precision >>0.1%). Opcja (c) nieprawda (monthly aggregation wouldn't be affected by single DST event). Opcja (d) irrelevant (drift affects raw too, nie aggregation bias)."
    }
  ]}
/>

:::tip Rekomendacja po quizie
Je≈õli uzyska≈Çe≈õ <80%, przejrzyj: (1) Noise reduction ‚àöN (CLT), (2) Aggregation functions (mean vs. max, context-dependent), (3) Retention tier selection (query optimization). Windowing i downsampling to core skills data engineeringu.
:::

</Slide>

</SlideContainer>

---

## Podsumowanie i wnioski

**Kluczowe punkty z tej sekcji:**

**Windowing (wyg≈Çadzanie)**:
   - **Noise reduction**: ‚àù ‚àöN (60 s window ‚Üí 7.7√ó reduction)
   - **Types**: Rectangular (simple), Hanning/Hamming (spectral analysis), EWMA (adaptive)
   - **Trade-off**: Smoothing (FP reduction) vs. Detection latency (delay)
   - **Rule**: Window size = 3-5√ó process time constant

**Resampling (multi-rate synchronization)**:
   - **Downsampling**: Aggregation (mean, max, sum, p95) ‚Äì context-dependent choice
   - **Upsampling**: Interpolation (forward-fill, linear) ‚Äì synthetic data, flag accordingly
   - **Alignment**: Common time grid (resample + reindex) kluczowy dla correlation

**Retention cascade (storage optimization)**:
   - **4 tiers**: Raw (7d), 1-min (6mo), 15-min (3y), Hourly (20y)
   - **Storage savings**: 200√ó reduction (2.5 TB ‚Üí 12 GB) bez significant accuracy loss
   - **Automation**: Retention policies + Continuous Queries (InfluxDB, TimescaleDB)
   - **Query optimization**: Auto-select tier based on time range

**Best practices**:
   - **Separate windows** dla alarming (small, fast) vs. KPI (large, stable)
   - **15-min minimum** dla long-term compliance (grid telemetry standard)
   - **Test aggregation accuracy** before deploying retention (PR error <0.05% target)

**Nastƒôpne kroki:**
- ƒÜwiczenie: Implement retention cascade (InfluxDB config)
- Lab: Window size optimization (FP rate vs. latency trade-off)
- Przygotowanie: Nastƒôpna sekcja ‚Äì TSDB (Time-Series Databases) wyb√≥r i konfiguracja

---

**Dodatkowe zasoby:**
- **Pandas time series**: Documentation (`resample`, `rolling`, `ewm`)
- **InfluxDB**: Retention policies, Continuous Queries guide
- **DSP literature**: Window functions (Harris, 1978), Signal processing textbooks


